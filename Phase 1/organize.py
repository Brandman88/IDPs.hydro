# Import necessary libraries. These provide functionalities for various tasks such as:
# subprocess: Running shell commands from Python
# time: Time-related functions
# datetime: Date-related functions
# shutil: High-level file operations like copying and removal
# os: Interacting with the operating system
# math and random: Mathematical functions and random number generation
# numpy: Scientific computing and array manipulation
#import subprocess
#import time
import datetime
import shutil
import os
from math import *
from random import *
import numpy as np
import pandas as pd
import glob
import matplotlib.pyplot as plt
import re

parameters="multi_run.dat"
#num_run is first number, says how many runs are we doing?
#start is the second number, says where we are in that list
#marker is are we at the start or end of a task. default is S for starting


# define file names and directories
dest_file="1LC.txt"
dest_file_debug="debug_file.txt"
dest_file_OLC="OneLetterCode.dat"
dest_file_TLC="ThreeLetterCode.dat"
dest_required_file="stats_module.dat"
dest_required_file_csv="data.csv"
cur_dir=os.getcwd()
dir_comp=f"{cur_dir}/completed_run"

# function to find protein length
def find_Protein_length(dest_file_OLC='OneLetterCode.dat'):
    # check if OneLetterCode file exists, if not, generate it
    if not os.path.exists(dest_file_OLC):
        onelettercode_generate()
    # compute sequence length by loading three letter code data file
    seq_length = len(np.loadtxt('ThreeLetterCode.dat', dtype="U", unpack=True))
    return seq_length

# This function generates the three-dimensional structure of a protein based on a given sequence. 
# If the required files do not exist, they are generated by calling other functions.
# The function also determines the length of the protein, then calculates and stores the (x, y, z) positions of each amino acid in the protein.
# Additionally, it checks for any amino acids that are too close to each other.
# Finally, it reads in force field parameters for all amino acids and translates the sequence into a numerical code.
def chain3d_generate(dest_file_TLC='ThreeLetterCode.dat'):
    # Checks if the ThreeLetterCode file exists. If not, it checks for the OneLetterCode file.
    # If OneLetterCode file doesn't exist either, it creates it. Otherwise, it generates the ThreeLetterCode file.
    if not os.path.exists(dest_file_TLC):
        if not os.path.exists(dest_file_OLC):
            onelettercode_generate()
        else:
            threelettercode_generate()
        
    # Determine the length of the protein.
    ParticleN = find_Protein_length()
    print ("IDP Length:", ParticleN)

    # Open a new file to store the (x, y, z) positions of each amino acid in the protein.
    fi = open (str(ParticleN),"w")

    # Define the spacing between amino acids in the protein.
    spacex = 0.38

    # Initialize the lists to store the (x, y, z) positions.
    x=[]
    y=[]
    z=[]

    # Set the initial position of the first amino acid.
    x.append(10.0)
    y.append(0.0)
    z.append(0.0)

    # Loop through the rest of the amino acids in the protein.
    # For each one, generate a random x position, set the y position to 0, and add the positions to their respective lists.
    i = 1
    while (i<ParticleN):
        xi = uniform(0,0.02)
        yi = 0.0
        x.append (x[0]+spacex*i)
        y.append (y[0]+yi)
        z.append (0.0)
        i=i+1

    # Print the (x, y, z) positions for each amino acid both in console and to the file.
    for i in range (ParticleN):
        print (x[i],y[i],z[i])
        print (x[i],y[i],z[i], file = fi)

    # Check if any two amino acids are too close to each other.
    # If two amino acids are closer than the defined spacing minus 0.1, print a warning.
    for i in range (ParticleN):
        for j in range (ParticleN):
            if (i != j):
                r = sqrt((x[i]-x[j])*(x[i]-x[j])+(y[i]-y[j])*(y[i]-y[j]))
                if (r < spacex-0.1):
                    print ("------------------------------------------------")
                    print (i, j, r)
                
    # Load the force field parameters for all amino acids.
    ff_para = 'stats_module.dat'
    aalist={}
    with open(ff_para,'r') as fid:
        for i in fid:
            if i[0]!='#':
                tmp=i.rsplit()
                aalist[tmp[0]]=np.loadtxt(tmp[1:],dtype=float)
    aakeys=list(aalist.keys())

    # Translate the amino acid sequence into a numerical code according to the


    # This translates each amino acid type into a number, which will be used in HOOMD
    # For example, GLY is with an ID of 10

    aamass=[]
    aacharge=[]
    aasigma=[]
    aahps=[]
    print ("List of Amino Acids:", aakeys)

    for i in aakeys:
        aamass.append(aalist[i][0])
        aacharge.append(aalist[i][1])
        aasigma.append(aalist[i][2])
        aahps.append(aalist[i][3])

    # Now we can translate the entire sequence into a number code according to the order in 'aakeys'

    fi_param = open ("chain_param.dat","w")
    with open('ThreeLetterCode.dat','r') as fid:
        for i in fid:
            iname=i.rsplit()[0]
            print(iname, aakeys.index(iname), aalist[iname][0], aalist[iname][1], aalist[iname][2], aalist[iname][3], file = fi_param)
        
    fid.close()
    fi_param.close()

# This function translates a one-letter protein sequence into a three-letter sequence.
# It first checks if the ThreeLetterCode file exists. If it does not, it creates a new empty one.
# Then, it checks if the OneLetterCode file exists. If it does not, it creates it by calling the function onelettercode_generate().
# After that, it reads the OneLetterCode sequence and translates each letter into the corresponding three-letter code.
# The translation is based on a dictionary where each key-value pair represents a one-letter and a three-letter code, respectively.
# The translated sequence is saved into the ThreeLetterCode file.
def threelettercode_generate(dest_file_OLC='OneLetterCode.dat',dest_file_TLC='ThreeLetterCode.dat',dest_file='1LC.txt'):
    # Checks if the ThreeLetterCode file exists. If not, it creates a new one.
    if not os.path.exists(dest_file_TLC):
        f = open(dest_file_TLC, 'x')
        f.close()
    
    # Checks if the OneLetterCode file exists. If not, it generates it.
    if not os.path.exists(dest_file_OLC):
        onelettercode_generate()
    
    # The dictionary for one-letter to three-letter amino acid code translation.
    seq={'R':'ARG','H':'HIS','K':'LYS','D':'ASP','E':'GLU',
         'S':'SER','T':'THR','N':'ASN','Q':'GLN','C':'CYS',
         'U':'SEC','G':'GLY','P':'PRO','A':'ALA','V':'VAL',
         'I':'ILE','L':'LEU','M':'MET','F':'PHE','Y':'TYR',
         'W':'TRP'} 
    
    # Read the one-letter sequence from the file.
    oneletter_seq = open(dest_file_OLC, "r").read()
    
    # Determine the length of the sequence.
    seq_length = len(oneletter_seq)
    print ("One Letter Sequence:", oneletter_seq)
    print ("Length of the protein:", seq_length)
    
    # Open the ThreeLetterCode file to write the translated sequence.
    threeletter_seq = open(dest_file_TLC, "w")
    
    # Translate each one-letter code into a three-letter code and write it to the file.
    for i in range (seq_length):
        seq_key = oneletter_seq[i]
        if (seq_key != '\n'):
            print (i+1, seq_key, seq[seq_key])
            print (seq[seq_key], file = threeletter_seq)      
    threeletter_seq.close()       

# This function reads a sequence from a file and extracts all the alphabetic characters to create a one-letter-code sequence.
# If the OneLetterCode file does not exist, it creates it.
# The function reads the sequence one character at a time and writes each alphabetic character to the OneLetterCode file.
def onelettercode_generate(dest_file_OLC='OneLetterCode.dat',dest_file='1LC.txt'):
    # Check if the OneLetterCode file exists. If not, create a new one.
    if not os.path.exists(dest_file_OLC):
        f = open(dest_file_OLC, 'x')
        f.close()
    
    # Open the input file to read the sequence and the OneLetterCode file to write the one-letter-code sequence.
    file = open(dest_file, 'r')
    fi = open (dest_file_OLC, 'w')
    
    # Read the sequence one character at a time.
    while True: 
        # Read the next character.
        char = file.read(1)
        
        # If we've reached the end of the file, break out of the loop.
        if not char:
            break
        
        # If the character is an alphabet letter (i.e., a part of the protein sequence), write it to the OneLetterCode file.
        elif char.isalpha():
            print(char, file = fi)
    
    # Close the files after reading and writing are done.
    file.close()
    fi.close()
        
# This function checks if a specified parameter file exists. If not, it creates the file and writes default values into it.
def check_multi(parameters,marker='S'):
    # If the specified parameter file does not exist, create it.
    if not os.path.exists(parameters):
        f = open(parameters, 'x')
        # Write default values into the newly created parameter file.
        f.write('1 #Number of proteins you care to run')
        f.write('\n1 #Start value (if you are just starting the process should be on 1 by default)')
        f.write(f'\n{marker} #Put S or E based on starting or end process (don\'t really need to touch), if its I then set the other 2 numbers and run the file again.') 
        f.close()# ^ the exception to S or E is I for initializing, just makes sure it's all there.

# This function checks if a specified destination file exists. If not, it creates the file.
def check_dest_file(dest_file='1LC.txt'):
    # If the specified destination file does not exist, create it.
    if not os.path.exists(dest_file):
        f = open(dest_file, 'x')
        f.close()

# This function checks if a specified debug file exists. If not, it creates the file.
def check_dest_file_debug(dest_file_debug="debug_file.txt"):
    # If the specified debug file does not exist, create it.
    if not os.path.exists(dest_file_debug):
        f = open(dest_file_debug, 'x')
        f.close()
        
# This function reads a multi-run parameter file and extracts the parameters from it.
def read_multi_run(parameters='multi_run.dat'):
    # Open the specified parameter file.
    with open(parameters,'r') as para:
        lines = []
        # Read the file line by line and store each line in a list.
        for line in para:
            lines.append(line)
    para.close()
    num_run_raw=lines[0]
    start_raw=lines[1]
    marker_raw=lines[2]
    
    # Extract the number of proteins to run.
    if '#Number of proteins you care to run' in num_run_raw:
        num_run=int(num_run_raw.replace('#Number of proteins you care to run','').strip())
    else:
        num_run=int(num_run_raw)

    # Extract the start value.
    if '#Start value (if you are just starting the process should be on 1 by default)' in start_raw:
        start=int(start_raw.replace('#Start value (if you are just starting the process should be on 1 by default)','').strip())
    else:
        start=int(start_raw)

    # Extract the marker.
    if '#Put S or E based on starting or end process (don\'t really need to touch), if its I then set the other 2 numbers and run the file again.' in marker_raw:
        marker=marker_raw.replace('#Put S or E based on starting or end process (don\'t really need to touch), if its I then set the other 2 numbers and run the file again.','')
    else:
        marker=marker_raw
    marker=marker.strip()

    # Remove the lines that have been processed.
    lines.remove(num_run_raw)
    lines.remove(start_raw)
    lines.remove(marker_raw)
  
    clean_list=[]
  
    # Go through the remaining lines and clean them up.
    for line in lines:
        line=line.strip()
        # If the line is not empty,
        if line.strip():
            # If the line contains a newline character, remove it.
                if '\n' in line:
                    # Remove newline character and any leading or trailing white spaces
                    line=line.replace('\n','').strip()
                    clean_list.append(line)
                else:
                    # Add the line into the clean list after removing leading or trailing white spaces
                    clean_list.append(line.strip())
                
    # Return the number of runs, start value, marker, and the cleaned list of lines
    return num_run,start,marker,clean_list

def edit_multi_run(parameters,num_run,start,marker,list_files):
    # Open the parameter file for writing
    with open(parameters,'w') as para:
        # Check marker status and edit start and marker accordingly
        if marker=='S':
            marker='E'
        elif marker=='E':
            start+=1
            marker='S'
            
        # Write the number of runs, start value, and marker to the file
        para.write(f'{num_run} #Number of proteins you care to run')
        para.write(f'\n{start} #Start value (if you are just starting the process should be on 1 by default)')
        para.write(f'\n{marker} #Put S or E based on starting or end process (don\'t really need to touch), if its I then set the other 2 numbers and run the file again.')

        # Write all the remaining files from the list_files to the parameter file
        for filing in list_files:
            para.write(f'\n{filing}')
    # Close the file
    para.close()
    
def cap_original_files(parameters='multi_run.dat'):
    # Get the current directory
    cur_dir=os.getcwd()
    
    # Get a list of all files in the current directory
    original_files = [f for f in os.listdir(cur_dir) if os.path.isfile(os.path.join(cur_dir, f))]
    
    # Append all original files to the parameter file
    with open(parameters,'a') as para:
        for o_file in original_files:
            para.write(f'\n{o_file}')
    
    # Close the file
    para.close()

def check_def_dir():
    # Get the current directory
    cur_dir=os.getcwd()
    
    # Define the paths to the run and completed directories
    dir_comp=f'{cur_dir}/completed_run'
  
    # Check if the directories exist, if not, create them
    if not os.path.exists(dir_comp):
        os.mkdir(dir_comp)
                     
def organized_end_dir_creator(start):
    # Get current directory
    Protein_Name=row_specific_info_csv(start,'Name')
    cur_dir=os.getcwd()
    # Define directory for completed run
    dir_comp=f'{cur_dir}/completed_run'
    # Define the end directory name
    end_dir_name=f'{dir_comp}/{Protein_Name}/'
    # Check if the directory exists
    if not os.path.exists(end_dir_name):
        # If not, create it
        os.mkdir(end_dir_name)
    else:
        end_dir_name=f'{dir_comp}/{Protein_Name}'
        i=2
        name_check=end_dir_name
        while True:
            name_check=f'{end_dir_name}_{i}/' 
            if not os.path.exists(name_check):#checking to make sure does not OverWrite dir info
                os.mkdir(name_check)
                end_dir_name=name_check
                break
            else:
                i+=1
    # Return the end directory name
    return end_dir_name

def change_e_to_s(parameters,num_run,start,marker,list_files,dest_file='1LC.txt'):
    # Create end directory and get its path
    current_end_dir=organized_end_dir_creator(start)
    # Get current directory
    cur_dir=os.getcwd()
    # Define the location of the one letter file
    location_One_letter=f"{cur_dir}/{dest_file}"
    # Define the end directory file for one letter
    end_dir_file_One_letter=f"{current_end_dir}/{dest_file}"
    # Copy the one letter file to the end directory
    shutil.copy2(location_One_letter,end_dir_file_One_letter)
    # Get a list of all files in the current directory
    current_list_of_files= [f for f in os.listdir(cur_dir) if os.path.isfile(os.path.join(cur_dir, f))]
    # Create a copy of the list
    files_in_need_for_transportation=current_list_of_files[:]
    # Edit multi run parameters
    edit_multi_run(parameters,num_run,start,marker,list_files)
    # Loop over all files in current directory
    for c_file in current_list_of_files:
        # And for all files in the list files
        for o_file in list_files:
            # If file from current directory matches a file from the list, remove it from the files needing transportation
            if c_file==o_file:
                files_in_need_for_transportation.remove(o_file)
    # Loop over all non default files
    for not_default_file in files_in_need_for_transportation:
        # Define the location of the non default file
        not_default_file_location=f'{cur_dir}/{not_default_file}'
        # Move the non default file to the end directory
        shutil.move(not_default_file_location,current_end_dir)

# This function checks whether the required directories and files exist and if not, creates them. 
# It then reads in the parameters from the specified file and runs the source file checker function.
# The source file name is also created based on the start value. The function returns the parameters read in.
def check_all(parameters):
    check_def_dir()  # Check if the default directories exist, create them if not.
    num_run,start,marker,list=read_multi_run(parameters)  # Read in parameters from the parameters file.
    marker=marker.strip()  # Remove any leading/trailing spaces from the marker.
    return num_run,start,marker,list

# I forgot comments on definitions have this format not the previous
def find_stokes_num_from_hoomd_file():
    """
    Scans through the current directory for files starting with 'hoomd', strips the string 'hoomd-' and '.out' from the file name and converts the resultant string to an integer.
    Returns this integer representing the hoomd number.
    """
    for file in os.listdir(os.getcwd()):  # Iterate over all files in the current directory
        if file.startswith("hoomd"):  # If file name starts with 'hoomd'
            file = file.replace('hoomd-', '').replace('.out', '').strip()  # Remove 'hoomd-' from the file name  # Remove '.out' from the file name
            hoomd_number = int(file)  # Convert file name to integer
    return hoomd_number

def remove_hoomd_file():
    """
    Iterates over all files in the current directory and removes the ones starting with 'hoomd'
    """
    for file in os.listdir(os.getcwd()):  # Iterate over all files in the current directory
        if file.startswith("hoomd"):  # If file name starts with 'hoomd'
            os.remove(file)  # Remove the file

def add_hoomd_file_multi(list):
    """
    Iterates over all files in the current directory and adds the one starting with 'hoomd' to a list
    """
    for item in list:
        if item.startswith("hoomd"):
            return list
    for file in os.listdir(os.getcwd()):  # Iterate over all files in the current directory
        if file.startswith("hoomd"):  # If file name starts with 'hoomd'
            list.append(file)  # Remove the file        
    return list

def packing_up():
    """
    Packages up the 'completed_run' directory into a zip file. The zip file is named with the Job number (hoomd number) and the date-time when the packing is done. The hoomd file is removed before packing.
    """
    cur_dir = os.getcwd()  # Get the current directory path
    cur_dt = datetime.datetime.now()  # Get the current date and time
    formatted_dt = cur_dt.strftime("%Y-%m-%d_%H-%M")  # Formatting date and time to the accepted naming convention
    stokes_num = find_stokes_num_from_hoomd_file()  # Get Stokes Job Number 
    remove_hoomd_file()  # Remove hoomd file
    parent_directory = os.path.dirname(cur_dir)  # Get the path of the parent directory
    completed_run_directory = os.path.join(cur_dir, 'completed_run')  # Path of the 'completed_run' directory
    zip_file_name = f'completed_run_Job({stokes_num})_TimeFinished({formatted_dt})'  # Define the zip file name
    zip_file_path = os.path.join(parent_directory, zip_file_name)  # Define the zip file path
    shutil.make_archive(zip_file_path, 'zip', completed_run_directory)  # Create the zip archive
    print(f"Zipped directory '{completed_run_directory}' and saved as '{zip_file_path}'")  # Inform the user of the completed task

# Special use-case for plotting current inside
def clean_up():
    """
    Removes specific files in the directory based on the file name patterns, and moves 'debug' file to another directory.
    """
    num_run, start, marker, list_files = read_multi_run()  # Reading details from multiple run
    if start > num_run:  # Only proceed if the starting number is greater than the total number of runs
        read_dat_files_data_merger_create_csv()
        # Special case for emergency plot goes here if post processing phase is incomplete
        current_list_of_files= [f for f in os.listdir(cur_dir) if os.path.isfile(os.path.join(cur_dir, f))]
        list_files=current_list_of_files[:]
        temp_list = list_files[:]  # Creating a temporary copy of the list of files
        for file in list_files:  # Iterate over each file in the list of files
            if file.endswith("py") or file.endswith("sh") or file.startswith("hoomd")or file==dest_required_file :  # If the file ends with 'py', equals to 'dest_required_file' or starts with 'hoomd'
                temp_list.remove(file)  # Remove such file from the temporary list
            elif file == dest_file_debug or file.endswith("csv") or file.endswith("svg"):  # If the file equals to 'dest_file_debug'
                temp_list.remove(file)  # Remove such file from the temporary list
                location_file = f"{cur_dir}/{file}"  # Define the current location of the debug file
                end_dir_file = f"{dir_comp}/{file}"  # Define the final location for the debug file
                shutil.move(location_file, end_dir_file)  # Move the debug file from current location to the end directory
        for file in temp_list:  # Iterate over the remaining files in the temporary list
            os.remove(file)  # Remove these remaining files from the directory
        packing_up()
        shutil.rmtree(dir_comp)  
        
def read_entire_csv_return_dict(dest_required_file_csv = "data.csv"):
    '''Define function to read entire CSV and return a dictionary.'''
    # Using pandas to read the CSV file located at the destination provided.
    df = pd.read_csv(dest_required_file_csv)

    # Converting the dataframe into a list of dictionaries where each dictionary represents a row of data.
    data = df.to_dict('records')

    # Return the list of dictionaries
    return data

def grab_info_from_csv_dict_row(start):
    '''Define function to grab information from a specific row in the CSV.'''
    # Call the function to read the CSV file and store the data in csv_dictionary
    csv_dictionary=read_entire_csv_return_dict()

    # Adjusting the row number to align with zero-indexed lists.
    csv_row_num=start-1 

    # Getting the information from the specific row in the CSV.
    current_row_info=csv_dictionary[csv_row_num]

    # Return the row information
    return current_row_info

def row_specific_info_csv(start,column_name):
    '''Define function to grab specific column information from a specific row in the CSV.'''
    # Call the function to get the information of the specific row
    current_row_info=grab_info_from_csv_dict_row(start)

    # Get the information of the specific column from the row.
    specific_csv_row_info=current_row_info[column_name]

    # Return the specific column information
    return specific_csv_row_info

def search_csv_sequence_return_name(file_location_protein_sequence,dest_required_file_csv = "data.csv"):
    '''Define function to grab specific column information from a specific row in the CSV.'''
    
    file = open(file_location_protein_sequence, 'r')
    read_sequence=file.readline()
    file.close()
    csv_dictionary=read_entire_csv_return_dict()
    i=0
    matching_rows=[]
    for row in csv_dictionary:
        if row['Sequence']==read_sequence:
            protein_name=row['Name']
            matching_rows.append(i)
        i+=1
    return protein_name,matching_rows

def write_csv_sequence_to_1LC(dest_file="1LC.txt"):
    num_run,start,marker,clean_list=read_multi_run()
    Protein_Sequence=row_specific_info_csv(start,'Sequence')
    with open(dest_file,'w') as file:
        # Check marker status and edit start and marker accordingly
        file.write(f'{Protein_Sequence}')
    # Close the file
    file.close()

def get_csv_headers(csv_file):
    """
    This function reads a CSV file and returns all the headers as a list.

    :param csv_file: Location of the CSV file
    :return: List of headers
    """
    try:
        df = pd.read_csv(csv_file)
        headers = df.columns.tolist()
        return headers
    except FileNotFoundError:
        print(f"File not found: {csv_file}")
        return []

def get_unique_and_matching_headers_relative_to_first_csv_file(csv_file1, csv_file2):
    """
    This function takes two CSV files, retrieves the headers from each file,
    and returns a list of headers that are in the first CSV file but not in the second CSV file.

    :param csv_file1: Location of the first CSV file
    :param csv_file2: Location of the second CSV file
    :return: List of unique headers in the first CSV file
    """
    headers1 = get_csv_headers(csv_file1)
    headers2 = get_csv_headers(csv_file2)

    unique_headers = [header for header in headers1 if header not in headers2]
    matching_headers=[header for header in headers1 if header in headers2]
    return unique_headers,matching_headers    
    
def read_dat_files_data_merger_create_csv(csv_file="data.csv", dir_comp=dir_comp):
    """
    This function reads .dat files from the given directory, merges them,
    and then merges the resulting DataFrame with data from a CSV file.

    :param csv_file: Location of the CSV file to merge data with
    :param dir_comp: The directory where the .dat files are located
    """

    # Use glob to get all the .dat files in subdirectories
    config_stat_files = glob.glob(f'{dir_comp}/**/config_stat.dat', recursive=True)
    quick_header_return_check_location=config_stat_files[0]
    unique_headers_rel1,shared_headers_rel1=get_unique_and_matching_headers_relative_to_first_csv_file(csv_file,quick_header_return_check_location)
    csv_data = pd.read_csv(csv_file)
    merged_data = csv_data.copy()
    # Loop through the files and read them in with pandas
    dataframes = []  # a list to hold all the individual pandas DataFrames
    for filename in config_stat_files:
        df = pd.read_csv(filename)  # assumes .dat files are CSV-formatted
        seq_file_loc = f'{os.path.basename(os.path.dirname(filename))}/{dest_file}'
        protein_name,matching_rows = search_csv_sequence_return_name(seq_file_loc)

        dir_name = os.path.basename(os.path.dirname(filename))
        dir_name_without_protein = dir_name.replace(protein_name, '').strip()

        # Extract the instance number from the directory name
        instance_number = None
        match = re.search(r'_(\d+)$', dir_name_without_protein)
        if match:
            instance_number = int(match.group(1))
            matching_array_value=instance_number-1
        else:
            matching_array_value=0

        non_matching_headers = set(df.columns) - set(merged_data.columns)
        for column in non_matching_headers:
            merged_data.loc[matching_rows[matching_array_value], column] = df.loc[0, column]
             
    # Write it out
    merged_data.to_csv('merged_config_stat_results.csv', index=False)
    
    
def plot_something_2locations_relative_to_one_variable(variable,loc1,loc2,dependent,independent):
    # Read the data from the CSV files
    data_df = pd.read_csv(loc1)
    config_df = pd.read_csv(loc2)  # Add subdirectory to file path

    # Merge the data based on matching names
    merged_df = pd.merge(data_df, config_df, on=variable)

    # Plot the data
    plt.scatter(merged_df[independent], merged_df[dependent])
    plt.xlabel(independent)
    plt.ylabel(dependent)
    plot_title=f'{dependent} vs {independent}'
    plt.title(plot_title)
    
    # Save the plot
    plt.savefig(f'{plot_title}.svg')
     
    
# Check if the parameters file exists
if not os.path.exists(parameters):
    # If the parameters file does not exist, initialize it with default values, and also initialize the destination and debug files.
    check_multi(parameters,'I')
    check_dest_file()
    check_dest_file_debug()
    
else:
    # If the parameters file exists, call the check_all function to set up the required directories/files and read in the parameters.
    num_run,start,marker,list_files=check_all(parameters)
    # If the start value is not greater than the number of runs, proceed
    if not start>num_run:
        # If the marker indicates initialization, write the parameters into the parameters file.
        if marker =='I':
            marker='S'
            with open(parameters,'w') as para:
                para.write(f'{num_run} #Number of proteins you care to run')
                para.write(f'\n{start} #Start value (if you are just starting the process should be on 1 by default)')
                para.write(f'\n{marker} #Put S or E based on starting or end process (don\'t really need to touch), if its I then set the other 2 numbers and run the file again.')
                para.close()   
            cap_original_files()  # Capture all original files
        # If the marker is 'S', then proceed with the generation of the three different codes and edit the parameters file.
        elif marker=='S':
            write_csv_sequence_to_1LC()  # Copy letter text
            onelettercode_generate()  # Generate one letter code
            threelettercode_generate()  # Generate three letter code
            chain3d_generate()  # Generate 3D chain
            list_files=add_hoomd_file_multi(list_files)
            edit_multi_run(parameters,num_run,start,marker,list_files)  # Edit the parameters file
        # If the marker is 'E', then move files to completed directory and edit the parameters file.
        elif marker=='E':
            change_e_to_s(parameters,num_run,start,marker,list_files)
            num_run,start,marker,list_files=read_multi_run()
            if start > num_run:
                clean_up()
        else:
            print('invalid marker')  # If the marker is not any of the valid values, print error message.
